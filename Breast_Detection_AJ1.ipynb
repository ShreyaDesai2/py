{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breast Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyaDesai2/py/blob/master/Breast_Detection_AJ1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwz_fFz39c16"
      },
      "source": [
        "The breast cancer database is a publicly available dataset from the UCI Machine learning Repository. The downloaded data set is .data file. The file extention can be changed to .csv file. The columns are named as ‘id’, ‘clump thickness’, ‘uniformity of cell size’, ‘uniformity of cell shape’, ‘marginal adhesion’, ‘single epithelial cell size’, ‘bare nuclei’, ‘bland chromatin’, ‘normal nucleoli’, ‘mitosis’ and ‘class’. This gives information on tumor features such as tumor size, density, and texture.\n",
        "The goal is then to create a classification model (mainly Support vector Classification) that looks at predicts if the cancer diagnosis is benign or malignant based on several features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbjNwZTJ38oz"
      },
      "source": [
        "# Step -1 - Import Package\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUX5eAm29lp4"
      },
      "source": [
        "Before applying any method, we need to check if any values are missing and then deal with them if so. In this dataset, there are no missing values. Then Data is divided into the Train set and Test set. We use the Train set to make the algorithm learn the data’s behavior and then check the accuracy of our model on the Test set.\n",
        "Features (X): The columns that are inserted into our model will be used to make predictions.\n",
        "Prediction (y): Target variable that will be predicted by the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG2uSO9U6M1k"
      },
      "source": [
        "# Step - 2 - Define the main function\n",
        "def main():\n",
        "    # Get data\n",
        "    cancer_data = load_breast_cancer()\n",
        "    cancer_data_X = pd.DataFrame(cancer_data.data, columns = cancer_data.feature_names)\n",
        "    cancer_data_y = cancer_data.target\n",
        "    features = cancer_data.feature_names\n",
        "    \n",
        "    vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness']\n",
        "    ## Data Exploration\n",
        "    print(f'The features in dataset are: {features}')\n",
        "    #print(f'Data description\\n {cancer_data_X.describe()}')\n",
        "    \n",
        "    #Plots\n",
        "    plot_data(cancer_data_X, cancer_data_y, features= vars, cor=True)\n",
        "\n",
        "    ## Remove Outliers\n",
        "    cancer_data_X, cancer_data_y = remove_outliers(cancer_data_X,cancer_data_y, features)\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = preprocess(cancer_data_X, cancer_data_y, features)\n",
        "    model = SVC(random_state=6)\n",
        "\n",
        "    model = train(model, X_train, y_train)\n",
        "    \n",
        "    baseline = evaluate(model, X_test, y_test, bl=True)\n",
        "\n",
        "    best_params = optimize_models(X_train, y_train)\n",
        "    print(best_params)\n",
        "\n",
        "    ## Build Best Model\n",
        "    best_C= best_params['C']\n",
        "    best_kernel = best_params['kernel']\n",
        "\n",
        "    best_model = SVC(kernel = best_kernel, C= best_C, random_state=6)\n",
        "    best_model = train(best_model, X_train, y_train)\n",
        "    evaluate (best_model, X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdJVdX9OZn2"
      },
      "source": [
        "Using the confusion matrix will give us important information: \n",
        "1. We had 171 women in our test set\n",
        "2. 4 women were classified as not having when actually they had (Type 1 error).\n",
        "3. 1 were classified as having breast cancer whey they did not (Type 2 error)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI-RA4nS7TvY"
      },
      "source": [
        "# Step - 3 - Plot graphs to understand data\n",
        "def plot_data(x_df, y_df,features, cor=False):\n",
        "    X = x_df.copy(deep=True)\n",
        "    X['class'] = y_df\n",
        "    sns.pairplot(X, hue = 'class', vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )\n",
        "    plt.show()\n",
        "    \n",
        "    if cor:\n",
        "      corr = X[features].corr()\n",
        "      plt.figure(figsize=(10,10))\n",
        "      sns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')\n",
        "      plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcwRk22w_mq0"
      },
      "source": [
        "So what can we do to improve our model ? \n",
        "First, let's remove the outliers from the database using the inplace function. secondly, we normalize the data set. Using the Feature scaling will help us to see all the variables from the same lens (same scale), in this way we will bring all values into the range [0,1]:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX7PFIOE7hnR"
      },
      "source": [
        "# Step - 4 - Preprocess data\n",
        "# Step -4a : Remove outliers\n",
        "def remove_outliers(x,y, features):\n",
        "    #remove null\n",
        "    x_df = x.copy(deep=True)\n",
        "    x_df['class'] = y\n",
        "    x_df.dropna(inplace=True)\n",
        "    return x_df[features], x_df['class']\n",
        "\n",
        "# Step -4b : Normalize data\n",
        "def scale_numeric(df):\n",
        "    x = df.values \n",
        "    scaler = preprocessing.StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled)\n",
        "    return df\n",
        "# Step -4b : Preprocess data\n",
        "def preprocess(x, y, features):\n",
        "    x_df = x[features].copy(deep=True)\n",
        "    x_df = scale_numeric(x_df)\n",
        "    #print(len(x_df),len(y))\n",
        "    # Split data into train, test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x_df,y, test_size=0.3, random_state=45)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gk5V-ikPkkI"
      },
      "source": [
        "So after training the model, what will the SVC model actually do? \n",
        "It’s important to start with the intuition for SVM with the special linearly separable classification case. If classification of observations is “linearly separable”, SVM fits the “decision boundary” that is defined by the largest margin between the closest points for each class. This is commonly called the “maximum margin hyperplane (MMH)”.\n",
        "\n",
        "Note that: In the training phase, the learning algorithm uses the training data to adjust the model’s parameters to minimize errors. At the end of the training phase, you get the trained model.\n",
        "In the testing phase, the trained model is applied to test data. Test data is separate from the training data, and is previously unseen by the model. The model is then evaluated on how it performs on the test data. The goal in building a classifier model is to have the model perform well on training as well as test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iTWpmES7oqK"
      },
      "source": [
        "# Step - 5 - train model \n",
        "def train(model,X_train, y_train):\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4j4nm83cdFn"
      },
      "source": [
        "as previously mentioned, we wanted to make sure that our classifier is efficiently categorizing the dataset in hand. To do it, we need performance metrics. Here we are as metrics: \n",
        "1. Confusion matrix: which specified the negatif and positive truly classified features.\n",
        "2. The accuracy or precision which specifies how many times the prediction is right. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQQCBmnw7vOX"
      },
      "source": [
        "# Step - 6 - Evaluate Model\n",
        "def evaluate(model, X_test, y_test, plot = True, print_results=True, bl=False):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
        "    acc = metrics.accuracy_score(y_test, y_pred)\n",
        "    if print_results:\n",
        "      if bl:\n",
        "        print('\\n\\nBaseline Model Performance on Test Dataset:\\n')\n",
        "      else:\n",
        "        print('\\n\\nBest Model Performance on Test Dataset:\\n')\n",
        "      print('\\nConfusion Matrix:\\n',cm)\n",
        "      print(f'Accuracy: {acc*100}%')\n",
        "\n",
        "    if plot:\n",
        "      sns.heatmap(cm, annot= True)\n",
        "      plt.show()\n",
        "    return \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFl1pIiWsjj"
      },
      "source": [
        "So now as we trained and tested our model, let's try to enhance it's accuracy. \n",
        "We will create a similar function to the function above used to cross validate and optimize the hyperparamters of the RBF kernel SVC and C. we noticed that we receive a 98% accuracy using the optimized hyperparameters C=5 along with the radial basis function.\n",
        "\n",
        "Conclusion: As you have seen SVMs easily get above 90% on this dataset. With just a bit of hyperparameter tuning we can push this number up to a remarkable 97%. Moreover, SVMs are a very simple algorithm and have taken no more than a few seconds to train on the computer. The potential to create models so easily that are this accurate on a task as complicated as breast cancer classification is perhaps one of the reasons that machine learning has become such a hot topic as of late."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaVlngTS7zJH"
      },
      "source": [
        "# Step - 7 - Improve Model\n",
        "def optimize_models(X_train, y_train):\n",
        "  params = {'kernel':['rbf'], 'C':[1.0, 5.0, 10]}\n",
        "  model = SVC(random_state=5)\n",
        "  clf = GridSearchCV(model, params)\n",
        "  clf.fit(X_train, y_train)\n",
        "  return clf.best_params_\n",
        "\n",
        "\n",
        "# call the main finction\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}